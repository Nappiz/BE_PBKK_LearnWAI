{"text": "Pengantar Perkuliahan dan \nPengenalan Data Mining\nwww.its.ac.id/informatikaDepartemen Teknik Informatika\n\nwww.its.ac.id/informatikaâ€¢Pengantar Perkuliahan\nâ€¢Deskripsi Matakuliah\nâ€¢Capaian Pembelajaran Matakuliah\nâ€¢Materi Perkuliahan\nâ€¢Deskripsi AsesmenPokok Bahasan\nwww.its.ac.id/informatikaDeskripsi Matakuliah\nNama MK : Data Mining\nKode MK : IF184951\nSKS / Semester : 3 /6\nDaftar Pustaka:\nâ€¢Utama\nâ€¢Pang -Ning Tan, Michael Steinbach, Anuj Karpatne , Vipin Kumar, \nâ€œIntroduction to Data Mining (Whatâ€™s New in Computer Science \nSeries)â€, 2nd edition, Pearson, 2019. \nâ€¢Pendukung\nâ€¢Jiawei Han, Micheline Kamber , â€œData Mining: Concept and \nTechniquesâ€, Morgan Kauffman Pub, 2001. \nâ€¢Anand Rajaraman, â€œMining of Massive Datasetsâ€, Standford\nUniversity, 2011.\n\nwww.its.ac.id/informatikaCapaian Pembelajaran\nMatakuliah\nMahasiswa mengetahui tentang bermacam -\nmacam tipe data dan beberapa sumber data\nMahasiswa memahami konsep dan mampu\nmenerapkan teknik pre-prosesing data\nMahasiswa mampu membuat sistem untuk\npenggalian data dan analisa pola data dengan\nmenerapkan metode -metode kecerdasan\nkomputasional dan metode probabilistik\nMahasiswa mampu menganalisis dan \nmenyelesaikan suatu permasalahan dalam suatu \nstu"}
{"text": "galian data dan analisa pola data dengan\nmenerapkan metode -metode kecerdasan\nkomputasional dan metode probabilistik\nMahasiswa mampu menganalisis dan \nmenyelesaikan suatu permasalahan dalam suatu \nstudi kasus dengan memanfaatkan sistem \npenggalian data\nwww.its.ac.id/informatikaMateri\n1.Konsep Dasar Data Mining \n2.Exploratory Data Analysis (EDA)\n3.Pre-processing\n4.Ensemble Learning \n5.Advanced Clustering\n6.Association Rule\n7.Deteksi Anomali\n8.Coding Python di Google Colab\n\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nMengapa Data Mining?\nïƒ¼Data yang dikumpulkan semakin bertambah banyak :\nâ€“Data web, e -commerce\nâ€“Data pembelian di toko/supermarket\nâ€“Transaksi Bank/ KartuKredit\nâ€“Detail panggilantelepon\nâ€“Statistik pemerintahan\nâ€“Rekam medis\nâ€“Database molekul\nâ€“Citra astronomis\n\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nMengapa Data Mining?\nïƒ¼Komputer sekarang murah dan powerful\nïƒ¼Tekanan kompetisi semakin kuat\nMenyediakan layanan yang lebih baik & customized \n(misal : dalam Customer Relationship Management )\nïƒ¼Teknik penyimpanan data semakin tinggi dengan kecepatan tinggi (GB/jam)\nïƒ¼Proses ekstraksi informasi di dala"}
{"text": "kan layanan yang lebih baik & customized \n(misal : dalam Customer Relationship Management )\nïƒ¼Teknik penyimpanan data semakin tinggi dengan kecepatan tinggi (GB/jam)\nïƒ¼Proses ekstraksi informasi di dalam kumpulan data yang besar menjadi\ntantangan\nïƒ¼Pendekatan analisis data tradisional tidak dapat digunakan untuk mengekstrak\ndata dalam jumlah besar\nïƒ¼Data mining merupakan teknik yang menggabungkan teknik analisis data \ntradisional dengan algoritma yang sophisticated untuk pemrosesan data dalam\njumlah besar\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nMengapa Data Mining?\nïƒ¼Informasi ygdibutuhkan dapat diekstrak menggunakan Data Mining \nmisalnya :\nâ€“Kebutuhan daripelanggan (data layanan telepon )\nâ€“Target responden untuk promosi produk\nâ€“Jenis produk yang paling banyak terjual\nâ€“Historis transaksi dariseorang pelanggan\nâ€“Pelanggan yang paling loyal\nâ€“Mengetahui perilaku gen-gen dalam berbagai situasi\nâ€“Mengetahui awal dan akhir musim pada suatu area\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nDefinisi Data Mining\nïƒ¼Mengekstrak atau â€œminingâ€ pengetahuan darikumpulan data yang \nsangat besar\nïƒ¼Ekstraksi informasi ygberguna d"}
{"text": "d/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nDefinisi Data Mining\nïƒ¼Mengekstrak atau â€œminingâ€ pengetahuan darikumpulan data yang \nsangat besar\nïƒ¼Ekstraksi informasi ygberguna daridata, dimana sebelumnya tidak\ndiharapkan , tidak dikenal dan implisit\nïƒ¼Eksplorasi dan analisis , secara otomatis atau semi -otomatis dari\nsekumpulan data ygsangat besar untuk memperoleh pola-pola data \nyang berarti\nïƒ¼Proses analisis database yang besar secara semi -otomatis untuk\nmenemukan pola yang valid, baru, berguna dan dapat dipahami\nmanusia\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nAkar Data Mining\nïƒ¼Berasal darimachine learning/AI, pattern \nrecognition, statistics, & database systems\nïƒ¼Teknik tradisional tidak sesuai disebabkan\nkarena :\nâ€“Data yang sangat besar\nâ€“Data dengan dimensi yang besar\nâ€“Data yang tersebar dan heterogenStatistics\nMachine \nLearning/AI\npattern \nrecognition\nData \nMining\nDatabaseInformasi\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nTantangan Data Mining\nïƒ¼Skalabilitas\nPenyimpanan data menggunakan ukuran besar (gigabyte, terabyte dst) maka pada proses \ndata mining, kumpulan data harus"}
{"text": "EKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nTantangan Data Mining\nïƒ¼Skalabilitas\nPenyimpanan data menggunakan ukuran besar (gigabyte, terabyte dst) maka pada proses \ndata mining, kumpulan data harus diskala\nïƒ¼Dimensionalitas ygtinggi\nData yang disimpan memiliki atribut dalam jumlah besar , contoh : data pengukuran suhu di \nbeberapa lokasi yang memiliki komponen waktu & spasial\nïƒ¼Data ygheterogen & kompleks\nData mempunyai atribut yang heterogen , contoh kumpulan halweb yang terdiri atas teks\nyang semi terstruktur & banyak terdapat link. Teknik data mining harus mempunyai\nteknik otokorelasi spasial & waktu , graph connectivity & keterhubungan parent -child \nantara elemen teks semi terstruktur & dokumen XML \nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nTantangan Data Mining\nïƒ¼Pendistribusian & kepemilikan data \nData tersimpan di lebih dari satu lokasi & dimiliki lebih dari satu organisasi . \nTeknik data mining harus dapat : \n(1) berkonsolidasi terhadap hasil data mining yang berasal dari sumber\nberbeda\n(2) mengurangi jumlah komunikasi untuk komputasi yang tersebar\nïƒ¼Analisis ygnon-tradisional , membutuhkan teknik evaluasi & penurunan\nribuan hipotesi"}
{"text": " hasil data mining yang berasal dari sumber\nberbeda\n(2) mengurangi jumlah komunikasi untuk komputasi yang tersebar\nïƒ¼Analisis ygnon-tradisional , membutuhkan teknik evaluasi & penurunan\nribuan hipotesis\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nProses Data Mining\nData mining merupakan bagian dariproses \nKnowledge Discovery in Databases (KDD)ïƒ \nProses transformasi data mentah menjadi informasi\nberguna . \nhttps://www.kdnuggets.com/gpspubs/aimag -kdd-\noverview -1996 -Fayyad.pdf\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nTahap pada KDD\n1. Selection\nPenyeleksian atau segmentasi data berdasarkan kriteria tertentu .\n2. Preprocessing\nTahap pembersihan data (info tidak berguna dibuang ), data dikonfigurasi ulang untuk menjamin format \ntetap konsisten .\n3. Transformation\nProses transformasi sehingga data dapat digunakan dan ditelusuri . Pemetaan data kompleks .\n4. Data mining\nProses ekstraksi pola dari data yang ada.\n5. Interpretation & evaluation\nProses interpretasi pola menjadi pengetahuan ygdapat digunakan untuk mendukung pengambilan\nkeputusan . \n(contoh : prediksi dan klasifikasi , ringkasan konten database se"}
{"text": "5. Interpretation & evaluation\nProses interpretasi pola menjadi pengetahuan ygdapat digunakan untuk mendukung pengambilan\nkeputusan . \n(contoh : prediksi dan klasifikasi , ringkasan konten database serta penjelasan fenomena yang diamati )\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nData Mining vs Bukan Data Mining\n1.Bukan DM: Pencarian informasi tertentu di internet\nDM : Pengelompokan informasi yang mirip dalam konteks tertentu\npada pencarian\n2.Bukan DM: Petugas medis mencari data medis untuk menganalisa catatan\ndengan penyakit tertentu\nDM : Peneliti medis mencari cara pengelompokan data penyakit\nberdasarkan data diagnosis, umur , alamat\n3.Bukan DM: Pembuatan laporan tahunan penjualan perusahaan dengan\nmerekap data selama setahun\nDM : Pemanfaatan data penjualan perusahaan untuk mendapatkan\npola prediksi yang sebaiknya untuk tahun berikutnya\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nTugas no.1\nAnda adalah seorang Data \nScientist/Analyst. Beri contoh\nsetiap task sebanyak 2 problem.\nâ€¢Classification\nâ€¢Clustering\nâ€¢Regression (forecasting)\nâ€¢Association Learning\nâ€¢Anomaly DetectionCONTOH PERUSAHAAN\nâ€¢PLN\nâ€¢PDAM\nâ€¢T"}
{"text": "orang Data \nScientist/Analyst. Beri contoh\nsetiap task sebanyak 2 problem.\nâ€¢Classification\nâ€¢Clustering\nâ€¢Regression (forecasting)\nâ€¢Association Learning\nâ€¢Anomaly DetectionCONTOH PERUSAHAAN\nâ€¢PLN\nâ€¢PDAM\nâ€¢Telkom\nâ€¢BPJS\nâ€¢Pertamina\nâ€¢Perbankan\nâ€¢Kantor Imigrasi\nâ€¢Rumah Sakit\nâ€¢Cloud service (google/AWS/Ms. Azure)\nâ€¢vendor seluler (telkomsel , indosat , XL, dll)\nâ€¢Kereta ApiIndonesia\nâ€¢Maskapai Penerbangan (Garuda Indonesia, Pelita Air, dll)\nâ€¢Perusahaan jurnalistik (Kompas , CNN dll)\nâ€¢Medsos (IG, Youtube , FB dll)\nwww.its.ac.id/informatika INSTITUT TEKNOLOGI SEPULUH NOPEMBER, Surabaya -Indonesia\nTugas no.2\nâ€¢Carilah dataset ( boleh tabular minimal 8 fitur, citra /gambar , teks, dll). \nâ€¢Jelaskan fitur-fiturnya dan penyelesaian problem pada dataset tersebut .\nMisal : data video lalulintas\n1. Untuk memprediksi kemacetan\n2. Untuk meng -klasifikasi jenis kendaraan\n3. dllx\nâ€¢Tampilkan dataset tersebut laluanalisis sederhana menggunakan python minimal \nberupa informasi dimensi , fitur, dan distribusi data.\n-TERIMA KASIH -\n\n01/22/2018 1Introduction to Data Mining, 2nd EditionData Mining: Data\nLecture Notes for Chapter 2\nIntroduction to Data Mining , 2nd Edition\nby\nTan, Steinbach, Karpatne, Kumar\n01/22/2018 2"}
{"text": "IMA KASIH -\n\n01/22/2018 1Introduction to Data Mining, 2nd EditionData Mining: Data\nLecture Notes for Chapter 2\nIntroduction to Data Mining , 2nd Edition\nby\nTan, Steinbach, Karpatne, Kumar\n01/22/2018 2Introduction to Data Mining, 2nd EditionOutline\nï¬Attributes and Objects\nï¬Types of Data\nï¬Data Quality\nï¬Similarity and Distance\nï¬Data Preprocessing\nWhat is Data?\nï¬Collection of data objects \nand their attributes\nï¬An attribute is a property or \ncharacteristic of an object\nâ€“Examples: eye color of a \nperson, temperature, etc.\nâ€“Attribute is also known as \nvariable, field, characteristic, \ndimension, or feature\nï¬A collection of attributes \ndescribe an object\nâ€“Object is also known as \nrecord, point, case, sample, \nentity, or instanceTid Refund Marital \nStatus Taxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 AttributesObjects\n01/22/2018 4Introduction to Data Mining, 2nd EditionA More Complete View of Data\nï¬Data may have parts\nï¬The different parts of the data may have \nrelationships \nï¬More generally, da"}
{"text": "10 AttributesObjects\n01/22/2018 4Introduction to Data Mining, 2nd EditionA More Complete View of Data\nï¬Data may have parts\nï¬The different parts of the data may have \nrelationships \nï¬More generally, data may have structure\nï¬Data can be incomplete\nï¬We will discuss this in more detail later\n01/22/2018 5Introduction to Data Mining, 2nd EditionAttribute Values\nï¬Attribute values are numbers or symbols \nassigned to an attribute for a particular object\nï¬Distinction between attributes and attribute values\nâ€“Same attribute can be mapped to different attribute \nvalues\nïµ Example: height can be measured in feet or meters\nâ€“Different attributes can be mapped to the same set of \nvalues\nïµ Example: Attribute values for ID and age are integers\nïµ But properties of attribute values can be different\nMeasurement of Length \nï¬The way you measure an attribute may not match the \nattributes properties.\n1\n2\n3\n55\n7\n8\n1510 4A\nB\nC\nD\nEThis scale \npreserves \nthe ordering \nand additvity \nproperties of \nlength.This scale \npreserves \nonly the \nordering \nproperty of \nlength.\n01/22/2018 7Introduction to Data Mining, 2nd EditionTypes of Attributes \nï¬ There are different types of attributes\nâ€“Nominal\nïµExamples: ID numbers, "}
{"text": "\npreserves \nonly the \nordering \nproperty of \nlength.\n01/22/2018 7Introduction to Data Mining, 2nd EditionTypes of Attributes \nï¬ There are different types of attributes\nâ€“Nominal\nïµExamples: ID numbers, eye color, zip codes\nâ€“Ordinal\nïµExamples: rankings (e.g., taste of potato chips on a \nscale from 1-10), grades, height {tall, medium, short}\nâ€“Interval\nïµExamples: calendar dates, temperatures in Celsius or \nFahrenheit.\nâ€“Ratio\nïµExamples: temperature in Kelvin, length, time, counts \n01/22/2018 8Introduction to Data Mining, 2nd EditionProperties of Attribute Values \nï¬The type of an attribute depends on which of the following \nproperties/operations it possesses:\nâ€“Distinctness: = ï‚¹\nâ€“Order: < > \nâ€“Differences are + - \nmeaningful : \nâ€“Ratios are * /\nmeaningful\nâ€“Nominal attribute: distinctness\nâ€“Ordinal attribute: distinctness & order\nâ€“Interval attribute: distinctness, order & meaningful differences\nâ€“Ratio attribute: all 4 properties/operations\n01/22/2018 9Introduction to Data Mining, 2nd EditionDifference Between Ratio and Interval \nï¬Is it physically meaningful to say that a \ntemperature of 10 Â° is twice that of 5 Â° on \nâ€“the Celsius scale?\nâ€“the Fahrenheit scale?\nâ€“the Kelvin scale?\nï¬Consider measur"}
{"text": "ence Between Ratio and Interval \nï¬Is it physically meaningful to say that a \ntemperature of 10 Â° is twice that of 5 Â° on \nâ€“the Celsius scale?\nâ€“the Fahrenheit scale?\nâ€“the Kelvin scale?\nï¬Consider measuring the height above average\nâ€“If Billâ€™s height is three inches above average and \nBobâ€™s height is six inches above average, then would \nwe say that Bob is twice as tall as Bill?\nâ€“Is this situation analogous to that of temperature?\n Attribute \nType Description \n Examples \n Operations \n \nNominal \n Nominal attribute \nvalues only \ndistinguish. (=, ï‚¹) zip codes, employee \nID numbers, eye \ncolor, sex: { male, \nfemale} mode, entropy, \ncontingency \ncorrelation, ï£2 \ntest \n Categorical \nQualitative \n \nOrdinal Ordinal attribute \nvalues also order \nobjects. \n(<, >) hardness of minerals, \n{good, better, best }, \ngrades, street \nnumbers median, \npercentiles, rank \ncorrelation, run \ntests, sign tests \nInterval For interval \nattributes, \ndifferences bet ween \nvalues are \nmeaningful. (+, - ) calendar dates, \ntemperature in \nCelsius or Fahrenheit mean, standard \ndeviation, \nPearson's \ncorrelation, t and \nF tests Numeric \nQuantitative \nRatio For ratio variables, \nboth differences and \nratios are \nmeaning"}
{"text": ", \ntemperature in \nCelsius or Fahrenheit mean, standard \ndeviation, \nPearson's \ncorrelation, t and \nF tests Numeric \nQuantitative \nRatio For ratio variables, \nboth differences and \nratios are \nmeaningful. (*, /) temperature in Kelvin, \nmonetary quantities, \ncounts, age, mass, \nlength, current geometric mean, \nharmonic mean, \npercent variation \nThis categorization of attributes is due to S. S. Stevens\n Attribute \nType Transformation \n Comments \n Categorical \nQualitative \n Nominal \n Any permutation of values \n If all employee ID numbers \nwere reassigned, would it \nmake any difference? \n \nOrdinal An order preserving change of \nvalues, i.e., \nnew_value = f(old_valu e) \nwhere f is a monotonic function \n An attribute encompassing \nthe notion of good, better best \ncan be represented equally \nwell by the values {1, 2, 3} or \nby { 0.5, 1, 10}. \n Numeric \nQuantitative Interval new_value = a * old_value + b \nwhere a and b are const ants Thus, the Fahrenheit and \nCelsius temperature scales \ndiffer in terms of where their \nzero value is and the size of a \nunit (degree). \nRatio new_value = a * old_value \n Length can be measured in \nmeters or feet. \n \nThis categorization of attributes is due to S"}
{"text": " in terms of where their \nzero value is and the size of a \nunit (degree). \nRatio new_value = a * old_value \n Length can be measured in \nmeters or feet. \n \nThis categorization of attributes is due to S. S. Stevens\n01/22/2018 12Introduction to Data Mining, 2nd EditionDiscrete and Continuous Attributes \nï¬Discrete Attribute\nâ€“Has only a finite or countably infinite set of values\nâ€“Examples: zip codes, counts, or the set of words in a \ncollection of documents \nâ€“Often represented as integer variables. \nâ€“Note: binary attributes are a special case of discrete \nattributes \nï¬Continuous Attribute \nâ€“Has real numbers as attribute values\nâ€“Examples: temperature, height, or weight. \nâ€“Practically, real values can only be measured and \nrepresented using a finite number of digits.\nâ€“Continuous attributes are typically represented as floating-\npoint variables. \n01/22/2018 13Introduction to Data Mining, 2nd EditionAsymmetric Attributes\nï¬Only presence (a non-zero attribute value) is regarded as important\nïµWords present in documents\nïµItems present in customer transactions\nï¬If we met a friend in the grocery store would we ever say the \nfollowing?\nâ€œI see our purchases are very similar since we didnâ€™t buy most"}
{"text": "s present in documents\nïµItems present in customer transactions\nï¬If we met a friend in the grocery store would we ever say the \nfollowing?\nâ€œI see our purchases are very similar since we didnâ€™t buy most of the same \nthings.â€ \nï¬We need two asymmetric binary attributes to represent one ordinary \nbinary attribute\nâ€“Association analysis uses asymmetric attributes\nï¬Asymmetric attributes typically arise from objects that are sets\n01/22/2018 14Introduction to Data Mining, 2nd EditionSome Extensions and Critiques\nï¬Velleman, Paul F., and Leland Wilkinson. \"Nominal, \nordinal, interval, and ratio typologies are misleading.\" The \nAmerican Statistician 47, no. 1 (1993): 65-72.\nï¬Mosteller, Frederick, and John W. Tukey. \"Data analysis \nand regression. A second course in statistics.\" Addison-\nWesley Series in Behavioral Science: Quantitative \nMethods, Reading, Mass.: Addison-Wesley, 1977.\nï¬Chrisman, Nicholas R. \"Rethinking levels of measurement \nfor cartography.\"Cartography and Geographic Information \nSystems 25, no. 4 (1998): 231-242. \n01/22/2018 15Introduction to Data Mining, 2nd EditionCritiques\nï¬Incomplete \nâ€“Asymmetric binary\nâ€“Cyclical\nâ€“Multivariate\nâ€“Partially ordered\nâ€“Partial membership\nâ€“Relatio"}
{"text": "stems 25, no. 4 (1998): 231-242. \n01/22/2018 15Introduction to Data Mining, 2nd EditionCritiques\nï¬Incomplete \nâ€“Asymmetric binary\nâ€“Cyclical\nâ€“Multivariate\nâ€“Partially ordered\nâ€“Partial membership\nâ€“Relationships between the data\nï¬Real data is approximate and noisy\nâ€“This can complicate recognition of the proper attribute type\nâ€“Treating one attribute type as another may be approximately \ncorrect\n01/22/2018 16Introduction to Data Mining, 2nd EditionCritiques â€¦\nï¬Not a good guide for statistical analysis\nâ€“May unnecessarily restrict operations and results \nïµStatistical analysis is often approximate\nïµThus, for example, using interval analysis for ordinal values \nmay be justified\nâ€“Transformations are common but donâ€™t preserve scales\nïµCan transform data to a new scale with better statistical \nproperties\nïµMany statistical analyses depend only on the distribution\n01/22/2018 17Introduction to Data Mining, 2nd EditionMore Complicated Examples\nï¬ID numbers \nâ€“Nominal, ordinal, or interval?\nï¬Number of cylinders in an automobile engine \nâ€“Nominal, ordinal, or ratio?\nï¬Biased Scale \nâ€“Interval or Ratio\n01/22/2018 18Introduction to Data Mining, 2nd EditionKey Messages for Attribute Types\nï¬The types of operati"}
{"text": "ders in an automobile engine \nâ€“Nominal, ordinal, or ratio?\nï¬Biased Scale \nâ€“Interval or Ratio\n01/22/2018 18Introduction to Data Mining, 2nd EditionKey Messages for Attribute Types\nï¬The types of operations you choose should be â€œmeaningfulâ€ for the \ntype of data you have\nâ€“Distinctness, order, meaningful intervals, and meaningful ratios are only \nfour properties of data\nâ€“The data type you see â€“ often numbers or strings â€“ may not capture all the \nproperties or may suggest properties that are not there\nâ€“Analysis may depend on these other properties of the data\nïµMany statistical analyses depend only on the distribution\nâ€“Many times what is meaningful is measured by statistical significance\nâ€“But in the end, what is meaningful is measured by the domain\n01/22/2018 19Introduction to Data Mining, 2nd EditionTypes of data sets \nï¬Record\nâ€“Data Matrix\nâ€“Document Data\nâ€“Transaction Data\nï¬Graph\nâ€“World Wide Web\nâ€“Molecular Structures\nï¬Ordered\nâ€“Spatial Data\nâ€“Temporal Data\nâ€“Sequential Data\nâ€“Genetic Sequence Data\n01/22/2018 20Introduction to Data Mining, 2nd EditionImportant Characteristics of Data\nâ€“Dimensionality (number of attributes)\nïµ High dimensional data brings a number of challenges\nâ€“Sparsity\nïµ Only "}
{"text": "ta\n01/22/2018 20Introduction to Data Mining, 2nd EditionImportant Characteristics of Data\nâ€“Dimensionality (number of attributes)\nïµ High dimensional data brings a number of challenges\nâ€“Sparsity\nïµ Only presence counts\nâ€“Resolution\nïµ Patterns depend on the scale \nâ€“Size\nïµType of analysis may depend on size of data\n01/22/2018 21Introduction to Data Mining, 2nd EditionRecord Data \nï¬Data that consists of a collection of records, each \nof which consists of a fixed set of attributes \nTid Refund Marital \nStatus Taxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n01/22/2018 22Introduction to Data Mining, 2nd EditionData Matrix \nï¬If data objects have the same fixed set of numeric \nattributes, then the data objects can be thought of as \npoints in a multi-dimensional space, where each \ndimension represents a distinct attribute \nï¬Such data set can be represented by an m by n matrix, \nwhere there are m rows, one for each object, and n \ncolumns, one for each attribute\n1.12.2 16.22 6.25 12.651.22.7 15.22 5.27 "}
{"text": "istinct attribute \nï¬Such data set can be represented by an m by n matrix, \nwhere there are m rows, one for each object, and n \ncolumns, one for each attribute\n1.12.2 16.22 6.25 12.651.22.7 15.22 5.27 10.23Thickness Load Distance Projection \nof y loadProjection \nof x Load\n1.12.2 16.22 6.25 12.651.22.7 15.22 5.27 10.23Thickness Load Distance Projection \nof y loadProjection \nof x Load\n01/22/2018 23Introduction to Data Mining, 2nd EditionDocument Data\nï¬Each document becomes a â€˜termâ€™ vector \nâ€“Each term is a component (attribute) of the vector\nâ€“The value of each component is the number of times \nthe corresponding term occurs in the document. \nDocument 1season\ntimeout\nlost\nwin\ngame\nscore\nball\nplay\ncoach\nteam\nDocument 2\nDocument 33050260202\n0\n0702100300\n100122030\n01/22/2018 24Introduction to Data Mining, 2nd EditionTransaction Data\nï¬A special type of record data, where \nâ€“Each record (transaction) involves a set of items. \nâ€“For example, consider a grocery store. The set of \nproducts purchased by a customer during one \nshopping trip constitute a transaction, while the \nindividual products that were purchased are the items. \nTID Items \n1 Bread, Coke, Milk \n2 Beer, Bread \n3 Beer, Coke, Diaper,"}
{"text": "y a customer during one \nshopping trip constitute a transaction, while the \nindividual products that were purchased are the items. \nTID Items \n1 Bread, Coke, Milk \n2 Beer, Bread \n3 Beer, Coke, Diaper, Milk \n4 Beer, Bread, Diaper, Milk \n5 Coke, Diaper, Milk \n \n01/22/2018 25Introduction to Data Mining, 2nd EditionGraph Data \nï¬Examples: Generic graph, a molecule, and webpages \n52\n1\n 2\n5\nBenzene Molecule: C6H6\n01/22/2018 26Introduction to Data Mining, 2nd EditionOrdered Data \nï¬Sequences of transactions\nAn element of \nthe sequenceItems/Events\n01/22/2018 27Introduction to Data Mining, 2nd EditionOrdered Data \nï¬ Genomic sequence data\nGGTTCCGCCTTCAGCCCCGCGCC\nCGCAGGGCCCGCCCCGCGCCGTC\nGAGAAGGGCCCGCCTGGCGGGCG\nGGGGGAGGCGGGGCCGCCCGAGC\nCCAACCGAGTCCGACCAGGTGCC\nCCCTCTGCTCGGCCTAGACCTGA\nGCTCATTAGGCGGCAGCGGACAG\nGCCAAGTAGAACACGCGAAGCGC\nTGGGCTGCCTGCTGCGACCAGGG\n01/22/2018 28Introduction to Data Mining, 2nd Edition\nOrdered Data\nï¬Spatio-Temporal Data\nAverage Monthly \nTemperature of \nland and ocean\n01/22/2018 29Introduction to Data Mining, 2nd EditionData Quality \nï¬Poor data quality negatively affects many data processing \nefforts\nâ€œThe most important point is that poor data quality is an unfolding \ndisaster"}
{"text": "9Introduction to Data Mining, 2nd EditionData Quality \nï¬Poor data quality negatively affects many data processing \nefforts\nâ€œThe most important point is that poor data quality is an unfolding \ndisaster.\nâ€“Poor data quality costs the typical company at least ten \npercent (10%) of revenue; twenty percent (20%) is \nprobably a better estimate.â€\nThomas C. Redman, DM Review, August 2004\nï¬Data mining example: a classification model for detecting \npeople who are loan risks is built using poor data\nâ€“Some credit-worthy candidates are denied loans\nâ€“More loans are given to individuals that default\n01/22/2018 30Introduction to Data Mining, 2nd EditionData Quality â€¦\nï¬What kinds of data quality problems?\nï¬How can we detect problems with the data? \nï¬What can we do about these problems? \nï¬Examples of data quality problems: \nâ€“Noise and outliers \nâ€“Missing values \nâ€“Duplicate data \nâ€“Wrong data\n01/22/2018 31Introduction to Data Mining, 2nd EditionNoise\nï¬For objects, noise is an extraneous object\nï¬For attributes, noise refers to modification of original values\nâ€“Examples: distortion of a personâ€™s voice when talking on a poor phone \nand â€œsnowâ€ on television screen\nTwo Sine Waves Two Sine Waves + Noise\n01/22/"}
{"text": "es, noise refers to modification of original values\nâ€“Examples: distortion of a personâ€™s voice when talking on a poor phone \nand â€œsnowâ€ on television screen\nTwo Sine Waves Two Sine Waves + Noise\n01/22/2018 32Introduction to Data Mining, 2nd Editionï¬Outliers are data objects with characteristics that \nare considerably different than most of the other \ndata objects in the data set\nâ€“Case 1: Outliers are \nnoise that interferes\nwith data analysis \nâ€“Case 2: Outliers are \nthe goal of our analysis\nïµ Credit card fraud\nïµ Intrusion detection \nï¬Causes?\nOutliers\n01/22/2018 33Introduction to Data Mining, 2nd EditionMissing Values\nï¬Reasons for missing values\nâ€“Information is not collected \n(e.g., people decline to give their age and weight)\nâ€“Attributes may not be applicable to all cases \n(e.g., annual income is not applicable to children)\nï¬Handling missing values\nâ€“Eliminate data objects or variables\nâ€“Estimate missing values\nïµExample: time series of temperature\nïµExample: census results \nâ€“Ignore the missing value during analysis\n01/22/2018 34Introduction to Data Mining, 2nd EditionMissing Values â€¦\nï¬Missing completely at random (MCAR)\nâ€“Missingness of a value is independent of attributes\nâ€“Fill in value"}
{"text": "ing value during analysis\n01/22/2018 34Introduction to Data Mining, 2nd EditionMissing Values â€¦\nï¬Missing completely at random (MCAR)\nâ€“Missingness of a value is independent of attributes\nâ€“Fill in values based on the attribute\nâ€“Analysis may be unbiased overall\nï¬Missing at Random (MAR)\nâ€“Missingness is related to other variables\nâ€“Fill in values based other values\nâ€“Almost always produces a bias in the analysis\nï¬Missing Not at Random (MNAR)\nâ€“Missingness is related to unobserved measurements\nâ€“Informative or non-ignorable missingness\nï¬Not possible to know the situation from the data\n01/22/2018 35Introduction to Data Mining, 2nd EditionDuplicate Data\nï¬Data set may include data objects that are duplicates, or \nalmost duplicates of one another\nâ€“Major issue when merging data from heterogeneous sources\nï¬Examples:\nâ€“Same person with multiple email addresses\nï¬Data cleaning\nâ€“Process of dealing with duplicate data issues\nï¬When should duplicate data not be removed?\n01/22/2018 36Introduction to Data Mining, 2nd EditionSimilarity and Dissimilarity Measures\nï¬Similarity measure\nâ€“Numerical measure of how alike two data objects are.\nâ€“Is higher when objects are more alike.\nâ€“Often falls in the range [0,1]\nï¬D"}
{"text": "g, 2nd EditionSimilarity and Dissimilarity Measures\nï¬Similarity measure\nâ€“Numerical measure of how alike two data objects are.\nâ€“Is higher when objects are more alike.\nâ€“Often falls in the range [0,1]\nï¬Dissimilarity measure\nâ€“Numerical measure of how different two data objects \nare \nâ€“Lower when objects are more alike\nâ€“Minimum dissimilarity is often 0\nâ€“Upper limit varies\nï¬Proximity refers to a similarity or dissimilarity\n01/22/2018 37Introduction to Data Mining, 2nd EditionSimilarity/Dissimilarity for Simple Attributes\nThe following table shows the similarity and dissimilarity \nbetween two objects, x and y, with respect to a single, simple \nattribute.\n\n01/22/2018 38Introduction to Data Mining, 2nd EditionEuclidean Distance\nï¬Euclidean Distance\n \nwhere n is the number of dimensions (attributes) and xk \nand yk are, respectively, the kth attributes \n(components) or data objects x and y.\nï¬ Standardization is necessary, if scales differ.\n\n01/22/2018 39Introduction to Data Mining, 2nd EditionEuclidean Distance\n0123\n0123456p1\np2p3 p4point x y\np1 0 2\np2 2 0\np3 3 1\np4 5 1\nDistance Matrixp1 p2 p3 p4\np1 02.828 3.162 5.099\np2 2.828 01.414 3.162\np3 3.162 1.414 0 2\np4 5.099 3.162 2 0\n01/22/2018 40Intr"}
{"text": "dean Distance\n0123\n0123456p1\np2p3 p4point x y\np1 0 2\np2 2 0\np3 3 1\np4 5 1\nDistance Matrixp1 p2 p3 p4\np1 02.828 3.162 5.099\np2 2.828 01.414 3.162\np3 3.162 1.414 0 2\np4 5.099 3.162 2 0\n01/22/2018 40Introduction to Data Mining, 2nd EditionMinkowski Distance\nï¬Minkowski Distance is a generalization of Euclidean \nDistance\n \n Where r is a parameter, n is the number of dimensions \n(attributes) and xk and yk are, respectively, the kth \nattributes (components) or data objects x and y.\n\n01/22/2018 41Introduction to Data Mining, 2nd EditionMinkowski Distance: Examples\nï¬r = 1. City block (Manhattan, taxicab, L1 norm) distance. \nâ€“A common example of this is the Hamming distance, which is just \nthe number of bits that are different between two binary vectors\nï¬r = 2. Euclidean distance\nï¬r ï‚® ï‚¥. â€œsupremumâ€ (Lmax norm, Lï‚¥ norm) distance. \nâ€“This is the maximum difference between any component of the \nvectors\nï¬Do not confuse r with n, i.e., all these distances are defined for \nall numbers of dimensions.\n01/22/2018 42Introduction to Data Mining, 2nd EditionMinkowski Distance\nDistance Matrixpoint xy\np102\np220\np331\np451L1p1p2p3p4\np10446\np24024\np34202\np46420\nL2p1p2p3p4\np1 02.8283.1625.099\np22.828 01.4143.1"}
{"text": "/22/2018 42Introduction to Data Mining, 2nd EditionMinkowski Distance\nDistance Matrixpoint xy\np102\np220\np331\np451L1p1p2p3p4\np10446\np24024\np34202\np46420\nL2p1p2p3p4\np1 02.8283.1625.099\np22.828 01.4143.162\np33.1621.414 02\np45.0993.162 20\nLï‚¥p1p2p3p4\np1 0235\np2 2013\np3 3102\np4 5320\n01/22/2018 43Introduction to Data Mining, 2nd EditionMahalanobis Distance\nFor red points, the Euclidean distance is 14.7, Mahalanobis distance is 6.ï“ is the covariance matrixğ¦ğšğ¡ğšğ¥ğšğ§ğ¨ğ›ğ¢ğ¬ (ğ±,ğ²)=(ğ±âˆ’ğ²)ğ‘‡\nÆ©âˆ’1\n(ğ±âˆ’ğ²)\n01/22/2018 44Introduction to Data Mining, 2nd EditionMahalanobis Distance\nCovariance \nMatrix:\nïƒº\nïƒ»ïƒ¹\nïƒª\nïƒ«ïƒ©\nï€½ï“\n3.02.02.03.0\nA: (0.5, 0.5)\nB: (0, 1)\nC: (1.5, 1.5)\nMahal(A,B) = 5\nMahal(A,C) = 4 \nB\nAC\n01/22/2018 45Introduction to Data Mining, 2nd EditionCommon Properties of a Distance\nï¬Distances, such as the Euclidean distance, \nhave some well known properties.\n1.d(x, y) ï‚³ 0 for all x and y and d(x, y) = 0 only if \nx = y. (Positive definiteness)\n2.d(x, y) = d(y, x) for all x and y. (Symmetry)\n3.d(x, z) ï‚£ d(x, y) + d(y, z) for all points x, y, and z. \n(Triangle Inequality)\nwhere d(x, y) is the distance (dissimilarity) between \npoints (data objects), x and y.\nï¬A distance that satisfies these properties is a \nmetr"}
{"text": "d(y, z) for all points x, y, and z. \n(Triangle Inequality)\nwhere d(x, y) is the distance (dissimilarity) between \npoints (data objects), x and y.\nï¬A distance that satisfies these properties is a \nmetric\n01/22/2018 46Introduction to Data Mining, 2nd EditionCommon Properties of a Similarity\nï¬Similarities, also have some well known \nproperties.\n1.s(x, y) = 1 (or maximum similarity) only if x = y. \n2.s(x, y) = s(y, x) for all x and y. (Symmetry)\nwhere s(x, y) is the similarity between points (data \nobjects), x and y.\n01/22/2018 47Introduction to Data Mining, 2nd EditionSimilarity Between Binary Vectors\nï¬Common situation is that objects, p and q, have only \nbinary attributes\nï¬Compute similarities using the following quantities\nf01 = the number of attributes where p was 0 and q was 1\nf10 = the number of attributes where p was 1 and q was 0\nf00 = the number of attributes where p was 0 and q was 0\nf11 = the number of attributes where p was 1 and q was 1\nï¬Simple Matching and Jaccard Coefficients \nSMC = number of matches / number of attributes \n = (f11 + f00) / (f01 + f10 + f11 + f00)\nJ = number of 11 matches / number of non-zero attributes\n = (f11) / (f01 + f10 + f11) \n01/22/2018 48Introduc"}
{"text": "s \nSMC = number of matches / number of attributes \n = (f11 + f00) / (f01 + f10 + f11 + f00)\nJ = number of 11 matches / number of non-zero attributes\n = (f11) / (f01 + f10 + f11) \n01/22/2018 48Introduction to Data Mining, 2nd EditionSMC versus Jaccard: Example\nx = 1 0 0 0 0 0 0 0 0 0 \ny = 0 0 0 0 0 0 1 0 0 1 \nf01 = 2 (the number of attributes where p was 0 and q was 1)\nf10 = 1 (the number of attributes where p was 1 and q was 0)\nf00 = 7 (the number of attributes where p was 0 and q was 0)\nf11 = 0 (the number of attributes where p was 1 and q was 1)\nSMC = (f11 + f00) / (f01 + f10 + f11 + f00)\n= (0+7) / (2+1+0+7) = 0.7 \nJ = (f11) / (f01 + f10 + f11) = 0 / (2 + 1 + 0) = 0 \n01/22/2018 49Introduction to Data Mining, 2nd EditionCosine Similarity\nï¬ If d1 and d2 are two document vectors, then\n cos( d1, d2 ) = <d1,d2> / ||d1|| ||d2|| , \nwhere <d1,d2> indicates inner product or vector dot \nproduct of vectors, d1 and d2, and || d || is the length of \nvector d. \nï¬ Example: \n d1 = 3 2 0 5 0 0 0 2 0 0 \n d2 = 1 0 0 0 0 0 0 1 0 2 \n<d1, d2> = 3*1 + 2*0 + 0*0 + 5*0 + 0*0 + 0*0 + 0*0 + 2*1 + 0*0 + 0*2 = 5\n| d1 || = (3*3+2*2+0*0+5*5+0*0+0*0+0*0+2*2+0*0+0*0)0.5 = (42) 0.5 = 6.481\n|| d2 || = (1*1+0*0+0*0"}
{"text": "0 \n d2 = 1 0 0 0 0 0 0 1 0 2 \n<d1, d2> = 3*1 + 2*0 + 0*0 + 5*0 + 0*0 + 0*0 + 0*0 + 2*1 + 0*0 + 0*2 = 5\n| d1 || = (3*3+2*2+0*0+5*5+0*0+0*0+0*0+2*2+0*0+0*0)0.5 = (42) 0.5 = 6.481\n|| d2 || = (1*1+0*0+0*0+0*0+0*0+0*0+0*0+1*1+0*0+2*2) 0.5 = (6) 0.5 = 2.449\ncos(d1, d2 ) = 0.3150\n01/22/2018 50Introduction to Data Mining, 2nd EditionExtended Jaccard Coefficient (Tanimoto)\nï¬Variation of Jaccard for continuous or count \nattributes\nâ€“Reduces to Jaccard for binary attributes\n\n01/22/2018 51Introduction to Data Mining, 2nd EditionCorrelation measures the linear relationship between objects\n\n01/22/2018 52Introduction to Data Mining, 2nd EditionVisually Evaluating Correlation\nScatter plots \nshowing the \nsimilarity from \nâ€“1 to 1.\n01/22/2018 53Introduction to Data Mining, 2nd EditionDrawback of Correlation\nï¬x = (-3, -2, -1, 0, 1, 2, 3)\nï¬y = (9, 4, 1, 0, 1, 4, 9)\nyi = xi2\nï¬mean(x) = 0, mean( y) = 4\nï¬std(x) = 2.16, std( y) = 3.74\nï¬corr = (-3)(5)+(-2)(0)+(-1)(-3)+(0)(-4)+(1)(-3)+(2)(0)+3(5) / ( 6 * 2.16 * 3.74 )\n = 0\n01/22/2018 54Introduction to Data Mining, 2nd EditionComparison of Proximity Measures\nï¬Domain of application\nâ€“Similarity measures tend to be specific to the type of attribute \nand data \nâ€“Re"}
{"text": ".74 )\n = 0\n01/22/2018 54Introduction to Data Mining, 2nd EditionComparison of Proximity Measures\nï¬Domain of application\nâ€“Similarity measures tend to be specific to the type of attribute \nand data \nâ€“Record data, images, graphs, sequences, 3D-protein \nstructure, etc. tend to have different measures\nï¬However, one can talk about various properties that you \nwould like a proximity measure to have\nâ€“Symmetry is a common one\nâ€“Tolerance to noise and outliers is another\nâ€“Ability to find more types of patterns? \nâ€“Many others possible\nï¬The measure must be applicable to the data and \nproduce results that agree with domain knowledge\n01/22/2018 55Introduction to Data Mining, 2nd EditionInformation Based Measures\nï¬Information theory is a well-developed and \nfundamental disciple with broad applications\nï¬Some similarity measures are based on \ninformation theory \nâ€“Mutual information in various versions\nâ€“Maximal Information Coefficient (MIC) and related \nmeasures\nâ€“General and can handle non-linear relationships\nâ€“Can be complicated and time intensive to compute\n01/22/2018 56Introduction to Data Mining, 2nd EditionInformation and Probability\nï¬Information relates to possible outcomes of an event \nâ€“transm"}
{"text": "nships\nâ€“Can be complicated and time intensive to compute\n01/22/2018 56Introduction to Data Mining, 2nd EditionInformation and Probability\nï¬Information relates to possible outcomes of an event \nâ€“transmission of a message, flip of a coin, or \nmeasurement of a piece of data \nï¬The more certain an outcome, the less information \nthat it contains and vice-versa\nâ€“For example, if a coin has two heads, then an outcome \nof heads provides no information\nâ€“More quantitatively, the information is related the \nprobability of an outcome\nïµThe smaller the probability of an outcome, the more information \nit provides and vice-versa\nâ€“Entropy is the commonly used measure\n\n01/22/2018 57Introduction to Data Mining, 2nd EditionEntropy\nï¬For \nâ€“a variable (event), X, \nâ€“with n possible values (outcomes), x1, x2 â€¦, xn \nâ€“each outcome having probability, p1, p2 â€¦, pn \nâ€“the entropy of X , H(X), is given by\nï¬Entropy is between 0 and log2n and is measured in \nbits\nâ€“Thus, entropy is a measure of how many bits it takes \nto represent an observation of X on average\n01/22/2018 58Introduction to Data Mining, 2nd EditionEntropy Examples\nï¬For a coin with probability p of heads and \nprobability q = 1 â€“ p of tails\nâ€“For p= 0.5,"}
{"text": " represent an observation of X on average\n01/22/2018 58Introduction to Data Mining, 2nd EditionEntropy Examples\nï¬For a coin with probability p of heads and \nprobability q = 1 â€“ p of tails\nâ€“For p= 0.5, q = 0.5 (fair coin) H = 1 \nâ€“For p = 1 or q = 1, H = 0 \nï¬What is the entropy of a fair four-sided die? \n01/22/2018 59Introduction to Data Mining, 2nd EditionEntropy for Sample Data: Example\nMaximum entropy is log25 = 2.3219Hair Color Count p -plog2p\nBlack 75 0.75 0.3113\nBrown 15 0.15 0.4105\nBlond 5 0.05 0.2161\nRed 0 0.00 0\nOther 5 0.05 0.2161\nTotal 100 1.0 1.1540\n01/22/2018 60Introduction to Data Mining, 2nd EditionEntropy for Sample Data\nï¬Suppose we have \nâ€“a number of observations ( m) of some attribute, X, \ne.g., the hair color of students in the class, \nâ€“where there are n different possible values\nâ€“And the number of observation in the ith category is mi\nâ€“Then, for this sample\nï¬For continuous data, the calculation is harder\n01/22/2018 61Introduction to Data Mining, 2nd EditionMutual Information\nï¬Information one variable provides about another\n Formally, , where\nH(X,Y) is the joint entropy of X and Y , \nWhere pij is the probability that the ith value of X and the jth value of Y \noccur"}
{"text": "rmation\nï¬Information one variable provides about another\n Formally, , where\nH(X,Y) is the joint entropy of X and Y , \nWhere pij is the probability that the ith value of X and the jth value of Y \noccur together \nï¬For discrete variables, this is easy to compute\nï¬Maximum mutual information for discrete variables is \nlog2(min( nX, nY ), where nX (nY) is the number of values of X (Y) \n01/22/2018 62Introduction to Data Mining, 2nd EditionMutual Information Example\nStudent \nStatus Countp-plog2p\nUndergrad450.450.5184\nGrad550.550.4744\nTotal1001.000.9928\nGradeCountp-plog2p\nA350.350.5301\nB500.500.5000\nC150.150.4105\nTotal1001.001.4406Student \nStatus GradeCountp-plog2p\nUndergradA50.050.2161\nUndergradB300.300.5211\nUndergradC100.100.3322\nGradA300.300.5211\nGradB200.200.4644\nGradC50.050.2161\nTotal 1001.002.2710\nMutual information of Student Status and Grade = 0.9928 + 1.4406 - 2.2710 = 0.1624\n01/22/2018 63Introduction to Data Mining, 2nd EditionMaximal Information Coefficient\nï¬Reshef, David N., Yakir A. Reshef, Hilary K. Finucane, Sharon R. Grossman, Gilean McVean, Peter J. \nTurnbaugh, Eric S. Lander, Michael Mitzenmacher, and Pardis C. Sabeti. \"Detecting novel \nassociations in large data sets.\" sc"}
{"text": ". Reshef, Hilary K. Finucane, Sharon R. Grossman, Gilean McVean, Peter J. \nTurnbaugh, Eric S. Lander, Michael Mitzenmacher, and Pardis C. Sabeti. \"Detecting novel \nassociations in large data sets.\" science 334, no. 6062 (2011): 1518-1524.\nï¬Applies mutual information to two continuous variables\nï¬Consider the possible binnings of the variables into \ndiscrete categories\nâ€“nX Ã— nY â‰¤ N0.6 where \nïµnX is the number of values of X\nïµnY is the number of values of Y\nïµN is the number of samples (observations, data objects)\nï¬Compute the mutual information\nâ€“Normalized by log2(min( nX, nY )\nï¬Take the highest value\n01/22/2018 64Introduction to Data Mining, 2nd Edition\nGeneral Approach for Combining Similarities\nï¬Sometimes attributes are of many different types, but an \noverall similarity is needed.\n1: For the kth attribute, compute a similarity, sk(x, y), in the \nrange [0, 1].\n2: Define an indicator variable, ï¤k, for the kth attribute as \nfollows:\nï¤k = 0 if the kth attribute is an asymmetric attribute and\n both objects have a value of 0, or if one of the objects \nhas a missing value for the kth attribute\nï¤k = 1 otherwise\n3. Compute\n01/22/2018 65Introduction to Data Mining, 2nd EditionUsing Weights "}
{"text": "and\n both objects have a value of 0, or if one of the objects \nhas a missing value for the kth attribute\nï¤k = 1 otherwise\n3. Compute\n01/22/2018 65Introduction to Data Mining, 2nd EditionUsing Weights to Combine Similarities\nï¬May not want to treat all attributes the same.\nâ€“Use non-negative weights ï€ \nï¬Can also define a weighted form of distance\n\n01/22/2018 66Introduction to Data Mining, 2nd EditionDensity\nï¬Measures the degree to which data objects are close to \neach other in a specified area\nï¬The notion of density is closely related to that of proximity\nï¬Concept of density is typically used for clustering and \nanomaly detection\nï¬Examples:\nâ€“Euclidean density\nïµ Euclidean density = number of points per unit volume\nâ€“Probability density\nïµ Estimate what the distribution of the data looks like\nâ€“Graph-based density\nïµ Connectivity\n01/22/2018 67Introduction to Data Mining, 2nd EditionEuclidean Density: Grid-based Approach\nï¬Simplest approach is to divide region into a \nnumber of rectangular cells of equal volume and \ndefine density as # of points the cell contains\n Grid-based density. Counts for each cell.\n\n01/22/2018 68Introduction to Data Mining, 2nd EditionEuclidean Density: Center-Based\nï¬Eu"}
{"text": "f equal volume and \ndefine density as # of points the cell contains\n Grid-based density. Counts for each cell.\n\n01/22/2018 68Introduction to Data Mining, 2nd EditionEuclidean Density: Center-Based\nï¬Euclidean density is the number of points within a \nspecified radius of the point\nIllustration of center-based density.\n01/22/2018 69Introduction to Data Mining, 2nd EditionData Preprocessing\nï¬Aggregation\nï¬Sampling\nï¬Dimensionality Reduction\nï¬Feature subset selection\nï¬Feature creation\nï¬Discretization and Binarization\nï¬Attribute Transformation\n01/22/2018 70Introduction to Data Mining, 2nd EditionAggregation\nï¬Combining two or more attributes (or objects) into \na single attribute (or object)\nï¬Purpose\nâ€“Data reduction\nïµ Reduce the number of attributes or objects\nâ€“Change of scale\nïµ Cities aggregated into regions, states, countries, etc.\nïµ Days aggregated into weeks, months, or years\nâ€“More â€œstableâ€ data\nïµ Aggregated data tends to have less variability \n01/22/2018 71Introduction to Data Mining, 2nd EditionExample: Precipitation in Australia\nï¬This example is based on precipitation in \nAustralia from the period 1982 to 1993. \nThe next slide shows \nâ€“A histogram for the standard deviation of average "}
{"text": "ditionExample: Precipitation in Australia\nï¬This example is based on precipitation in \nAustralia from the period 1982 to 1993. \nThe next slide shows \nâ€“A histogram for the standard deviation of average \nmonthly precipitation for 3,030 0.5â—¦ by 0.5â—¦ grid cells in \nAustralia, and\nâ€“A histogram for the standard deviation of the average \nyearly precipitation for the same locations.\nï¬The average yearly precipitation has less \nvariability than the average monthly precipitation. \nï¬All precipitation measurements (and their \nstandard deviations) are in centimeters.\n01/22/2018 72Introduction to Data Mining, 2nd EditionExample: Precipitation in Australia â€¦\nStandard Deviation of Average \nMonthly PrecipitationStandard Deviation of \nAverage Yearly Precipitation\nVariation of Precipitation in Australia\n01/22/2018 73Introduction to Data Mining, 2nd EditionSampling \nï¬Sampling is the main technique employed for data \nreduction.\nâ€“It is often used for both the preliminary investigation of \nthe data and the final data analysis.\n \nï¬Statisticians often sample because obtaining the \nentire set of data of interest is too expensive or time \nconsuming.\n \nï¬Sampling is typically used in data mining because \nprocess"}
{"text": "al data analysis.\n \nï¬Statisticians often sample because obtaining the \nentire set of data of interest is too expensive or time \nconsuming.\n \nï¬Sampling is typically used in data mining because \nprocessing the entire set of data of interest is too \nexpensive or time consuming.\n01/22/2018 74Introduction to Data Mining, 2nd EditionSampling â€¦ \nï¬The key principle for effective sampling is the \nfollowing: \nâ€“Using a sample will work almost as well as using the \nentire data set, if the sample is representative\nâ€“A sample is representative if it has approximately the \nsame properties (of interest) as the original set of data \n \n01/22/2018 75Introduction to Data Mining, 2nd EditionSample Size\n \n8000 points 2000 Points 500 Points\n01/22/2018 76Introduction to Data Mining, 2nd EditionTypes of Sampling\nï¬Simple Random Sampling\nâ€“There is an equal probability of selecting any particular item\nâ€“Sampling without replacement\nïµAs each item is selected, it is removed from the \npopulation\nâ€“Sampling with replacement\nïµObjects are not removed from the population as they are \nselected for the sample. \nïµIn sampling with replacement, the same object can be \npicked up more than once\nï¬Stratified sampling\nâ€“Split the"}
{"text": "ment\nïµObjects are not removed from the population as they are \nselected for the sample. \nïµIn sampling with replacement, the same object can be \npicked up more than once\nï¬Stratified sampling\nâ€“Split the data into several partitions; then draw random \nsamples from each partition\n01/22/2018 77Introduction to Data Mining, 2nd EditionSample Size\nï¬What sample size is necessary to get at least one \nobject from each of 10 equal-sized groups.\n\n01/22/2018 78Introduction to Data Mining, 2nd EditionCurse of Dimensionality\nï¬When dimensionality \nincreases, data becomes \nincreasingly sparse in the \nspace that it occupies\nï¬Definitions of density and \ndistance between points, \nwhich are critical for \nclustering and outlier \ndetection, become less \nmeaningful\nâ€¢Randomly generate 500 points\nâ€¢Compute difference between max and \nmin distance between any pair of points\n01/22/2018 79Introduction to Data Mining, 2nd EditionDimensionality Reduction\nï¬Purpose:\nâ€“Avoid curse of dimensionality\nâ€“Reduce amount of time and memory required by data \nmining algorithms\nâ€“Allow data to be more easily visualized\nâ€“May help to eliminate irrelevant features or reduce \nnoise\nï¬Techniques\nâ€“Principal Components Analysis (PCA)\nâ€“Si"}
{"text": "and memory required by data \nmining algorithms\nâ€“Allow data to be more easily visualized\nâ€“May help to eliminate irrelevant features or reduce \nnoise\nï¬Techniques\nâ€“Principal Components Analysis (PCA)\nâ€“Singular Value Decomposition\nâ€“Others: supervised and non-linear techniques\n01/22/2018 80Introduction to Data Mining, 2nd EditionDimensionality Reduction: PCA\nï¬Goal is to find a projection that captures the \nlargest amount of variation in data\nx2\nx1e\n01/22/2018 81Introduction to Data Mining, 2nd Edition\nDimensionality Reduction: PCA\n01/22/2018 82Introduction to Data Mining, 2nd EditionFeature Subset Selection\nï¬Another way to reduce dimensionality of data\nï¬Redundant features \nâ€“Duplicate much or all of the information contained in \none or more other attributes\nâ€“Example: purchase price of a product and the amount \nof sales tax paid\nï¬Irrelevant features\nâ€“Contain no information that is useful for the data \nmining task at hand\nâ€“Example: students' ID is often irrelevant to the task of \npredicting students' GPA\nï¬Many techniques developed, especially for \nclassification\n01/22/2018 83Introduction to Data Mining, 2nd EditionFeature Creation\nï¬Create new attributes that can capture the \nimportant info"}
{"text": "udents' GPA\nï¬Many techniques developed, especially for \nclassification\n01/22/2018 83Introduction to Data Mining, 2nd EditionFeature Creation\nï¬Create new attributes that can capture the \nimportant information in a data set much more \nefficiently than the original attributes\nï¬Three general methodologies:\nâ€“Feature extraction\nïµ Example: extracting edges from images\nâ€“Feature construction\nïµ Example: dividing mass by volume to get density \nâ€“Mapping data to new space\nïµ Example: Fourier and wavelet analysis \n01/22/2018 84Introduction to Data Mining, 2nd EditionMapping Data to a New Space\nTwo Sine Waves + Noise Frequencyï¬Fourier and wavelet transform\nFrequency\n01/22/2018 85Introduction to Data Mining, 2nd EditionDiscretization\nï¬Discretization is the process of converting a \ncontinuous attribute into an ordinal attribute\nâ€“A potentially infinite number of values are mapped into \n a small number of categories\nâ€“Discretization is commonly used in classification\nâ€“Many classification algorithms work best if both \nthe independent and dependent variables have \nonly a few values\nâ€“We give an illustration of the usefulness of \ndiscretization using the Iris data set\n01/22/2018 86Introduction to Data Mini"}
{"text": "st if both \nthe independent and dependent variables have \nonly a few values\nâ€“We give an illustration of the usefulness of \ndiscretization using the Iris data set\n01/22/2018 86Introduction to Data Mining, 2nd EditionIris Sample Data Set \nï¬Iris Plant data set.\nâ€“Can be obtained from the UCI Machine Learning Repository \nhttp://www.ics.uci.edu/~mlearn/MLRepository.html \nâ€“From the statistician Douglas Fisher\nâ€“Three flower types (classes):\nïµ Setosa\nïµ Versicolour\nïµ Virginica \nâ€“Four (non-class) attributes\nïµ Sepal width and length\nïµ Petal width and length Virginica. Robert H. Mohlenbrock. USDA \nNRCS. 1995. Northeast wetland flora: Field \noffice guide to plant species. Northeast National \nTechnical Center, Chester, PA. Courtesy of \nUSDA NRCS Wetland Science Institute. \n\nDiscretization: Iris Example\nPetal width low or petal length low implies Setosa.\nPetal width medium or petal length medium implies Versicolour.\nPetal width high or petal length high implies Virginica.\n01/22/2018 88Introduction to Data Mining, 2nd EditionDiscretization: Iris Example â€¦\nï¬How can we tell what the best discretization is?\nâ€“Unsupervised discretization: find breaks in the data \nvalues\nïµExample:\nPetal Length \nâ€“Supervis"}
{"text": "Data Mining, 2nd EditionDiscretization: Iris Example â€¦\nï¬How can we tell what the best discretization is?\nâ€“Unsupervised discretization: find breaks in the data \nvalues\nïµExample:\nPetal Length \nâ€“Supervised discretization: Use class labels to find \nbreaks 0246801020304050\nPetal LengthCounts\n01/22/2018 89Introduction to Data Mining, 2nd Edition\nDiscretization Without Using Class Labels \nData consists of four groups of points and two outliers. Data is one-\ndimensional, but a random y component is added to reduce overlap.\n01/22/2018 90Introduction to Data Mining, 2nd EditionDiscretization Without Using Class Labels \nEqual interval width approach used to obtain 4 values. \n01/22/2018 91Introduction to Data Mining, 2nd EditionDiscretization Without Using Class Labels \nEqual frequency approach used to obtain 4 values.\n\n01/22/2018 92Introduction to Data Mining, 2nd EditionDiscretization Without Using Class Labels \nK-means approach to obtain 4 values.\n\n01/22/2018 93Introduction to Data Mining, 2nd EditionBinarization\nï¬Binarization maps a continuous or categorical \nattribute into one or more binary variables\nï¬Typically used for association analysis\nï¬Often convert a continuous attribute to a \ncat"}
{"text": "EditionBinarization\nï¬Binarization maps a continuous or categorical \nattribute into one or more binary variables\nï¬Typically used for association analysis\nï¬Often convert a continuous attribute to a \ncategorical attribute and then convert a \ncategorical attribute to a set of binary attributes\nâ€“Association analysis needs asymmetric binary \nattributes\nâ€“Examples: eye color and height measured as \n{low, medium, high}\n01/22/2018 94Introduction to Data Mining, 2nd EditionAttribute Transformation\nï¬An attribute transform is a function that maps the \nentire set of values of a given attribute to a new set \nof replacement values such that each old value can \nbe identified with one of the new values\nâ€“Simple functions: xk, log(x), ex, |x|\nâ€“Normalization\nïµRefers to various techniques to adjust to differences \namong attributes in terms of frequency of occurrence, \nmean, variance, range\nïµTake out unwanted, common signal, e.g., seasonality \nâ€“In statistics, standardization refers to subtracting off the \nmeans and dividing by the standard deviation\n01/22/2018 95Introduction to Data Mining, 2nd EditionExample: Sample Time Series of Plant Growth\nCorrelations between time series\nMinneapolis\n Minneapolis At"}
{"text": "ans and dividing by the standard deviation\n01/22/2018 95Introduction to Data Mining, 2nd EditionExample: Sample Time Series of Plant Growth\nCorrelations between time series\nMinneapolis\n Minneapolis Atlanta Sao Paolo \nMinneapolis 1.0000 0.7591 -0.7581 \nAtlanta 0.7591 1.0000 -0.5739 \nSao Paolo -0.7581 -0.5739 1.0000 \n Correlations between time seriesNet Primary \nProduction (NPP) \nis a measure of \nplant growth used \nby ecosystem \nscientists.\n01/22/2018 96Introduction to Data Mining, 2nd EditionSeasonality Accounts for Much Correlation\nCorrelations between time series\nMinneapolis\nNormalized using \nmonthly Z Score:\nSubtract off monthly \nmean and divide by \nmonthly standard \ndeviation\n Minneapolis Atlanta Sao Paolo \nMinneapolis 1.0000 0.0492 0.0906 \nAtlanta 0.0492 1.0000 -0.0154 \nSao Paolo 0.0906 -0.0154 1.0000 \n Correlations between time series\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Data Mining: Exploring DataLecture Notes for Chapter 3Introduction to Data MiningbyTan, Steinbach, Kumar\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º What is data exploration?ï¬Key motivations of data exploration includeâ€“Helping to select the right tool for prepro"}
{"text": "Tan, Steinbach, Kumar\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º What is data exploration?ï¬Key motivations of data exploration includeâ€“Helping to select the right tool for preprocessing or analysisâ€“Making use of humansâ€™ abilities to recognize patternsu People can recognize patterns not captured by data analysis tools ï¬Related to the area of Exploratory Data Analysis (EDA)â€“Created by statistician John Tukeyâ€“Seminal book is Exploratory Data Analysis by Tukeyâ€“A nice online introduction can be found in Chapter 1 of the NIST Engineering Statistics Handbookhttp://www.itl.nist.gov/div898/handbook/index.htmA preliminary exploration of the data to better understand its characteristics.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Techniques Used In Data Exploration ï¬In EDA, as originally defined by Tukeyâ€“The focus was on visualizationâ€“Clustering and anomaly detection were viewed as exploratory techniquesâ€“In data mining, clustering and anomaly detection are major areas of interest, and not thought of as just exploratoryï¬In our discussion of data exploration, we focus onâ€“Summary statisticsâ€“Visualizationâ€“Online Analytical Processing (OLAP) \nÂ© Tan,Stein"}
{"text": "are major areas of interest, and not thought of as just exploratoryï¬In our discussion of data exploration, we focus onâ€“Summary statisticsâ€“Visualizationâ€“Online Analytical Processing (OLAP) \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Iris Sample Data Set ï¬Many of the exploratory data techniques are illustrated with the Iris Plant data set.â€“Can be obtained from the UCI Machine Learning Repository http://www.ics.uci.edu/~mlearn/MLRepository.html â€“From the statistician Douglas Fisherâ€“Three flower types (classes):u Setosau Virginica u Versicolourâ€“Four (non-class) attributesu Sepal width and lengthu Petal width and lengthVirginica. Robert H. Mohlenbrock. USDA NRCS. 1995. Northeast wetland flora: Field office guide to plant species. Northeast National Technical Center, Chester, PA. Courtesy of USDA NRCS Wetland Science Institute. \n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Summary Statisticsï¬Summary statistics are numbers that summarize properties of the dataâ€“Summarized properties include frequency, location and spreadu Examples: location - mean spread - standard deviationâ€“Most summary statistics can be calculated in a single pass through the da"}
{"text": "f the dataâ€“Summarized properties include frequency, location and spreadu Examples: location - mean spread - standard deviationâ€“Most summary statistics can be calculated in a single pass through the data\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Frequency and Modeï¬The frequency of an attribute value is the percentage of time the value occurs in the data set â€“For example, given the attribute â€˜genderâ€™ and a representative population of people, the gender â€˜femaleâ€™ occurs about 50% of the time.ï¬The mode of a an attribute is the most frequent attribute value ï¬The notions of frequency and mode are typically used with categorical data\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Percentilesï¬For continuous data, the notion of a percentile is more useful. Given an ordinal or continuous attribute x and a number p between 0 and 100, the pth percentile is a value of x such that p% of the observed values of x are less than . ï¬For instance, the 50th percentile is the value such that 50% of all values of x are less than . xp xp xp x50% x50%\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nMeasures of Location: Mean and Medianï¬The mean is "}
{"text": "le is the value such that 50% of all values of x are less than . xp xp xp x50% x50%\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nMeasures of Location: Mean and Medianï¬The mean is the most common measure of the location of a set of points. ï¬However, the mean is very sensitive to outliers. ï¬Thus, the median or a trimmed mean is also commonly used.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Measures of Spread: Range and Varianceï¬Range is the difference between the max and minï¬The variance or standard deviation is the most common measure of the spread of a set of points. ï¬However, this is also sensitive to outliers, so that other measures are often used. \n \n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º VisualizationVisualization is the conversion of data into a visual or tabular format so that the characteristics of the data and the relationships among data items or attributes can be analyzed or reported.ï¬Visualization of data is one of the most powerful and appealing techniques for data exploration. â€“Humans have a well developed ability to analyze large amounts of information that is presented visuallyâ€“Can detect general p"}
{"text": "is one of the most powerful and appealing techniques for data exploration. â€“Humans have a well developed ability to analyze large amounts of information that is presented visuallyâ€“Can detect general patterns and trendsâ€“Can detect outliers and unusual patterns \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nExample: Sea Surface Temperatureï¬The following shows the Sea Surface Temperature (SST) for July 1982â€“Tens of thousands of data points are summarized in a single figure \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Representationï¬Is the mapping of information to a visual formatï¬Data objects, their attributes, and the relationships among data objects are translated into graphical elements such as points, lines, shapes, and colors.ï¬Example: â€“Objects are often represented as pointsâ€“Their attribute values can be represented as the position of the points or the characteristics of the points, e.g., color, size, and shapeâ€“If position is used, then the relationships of points, i.e., whether they form groups or a point is an outlier, is easily perceived.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Arrangementï¬Is the placement of v"}
{"text": "elationships of points, i.e., whether they form groups or a point is an outlier, is easily perceived.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Arrangementï¬Is the placement of visual elements within a displayï¬Can make a large difference in how easy it is to understand the dataï¬Example: \n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Selectionï¬Is the elimination or the de-emphasis of certain objects and attributesï¬Selection may involve the chossing a subset of attributes â€“Dimensionality reduction is often used to reduce the number of dimensions to two or threeâ€“Alternatively, pairs of attributes can be consideredï¬Selection may also involve choosing a subset of objectsâ€“ A region of the screen can only show so many pointsâ€“Can sample, but want to preserve points in sparse areas \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization Techniques: Histogramsï¬Histogram â€“Usually shows the distribution of values of a single variableâ€“Divide the values into bins and show a bar plot of the number of objects in each bin. â€“The height of each bar indicates the number of objectsâ€“Shape of histogram depends on the number of binsï¬Example"}
{"text": "eâ€“Divide the values into bins and show a bar plot of the number of objects in each bin. â€“The height of each bar indicates the number of objectsâ€“Shape of histogram depends on the number of binsï¬Example: Petal Width (10 and 20 bins, respectively) \n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nTwo-Dimensional Histogramsï¬Show the joint distribution of the values of two attributes ï¬Example: petal width and petal lengthâ€“What does this tell us? \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization Techniques: Box Plotsï¬Box Plots â€“Invented by J. Tukeyâ€“Another way of displaying the distribution of data â€“Following figure shows the basic part of a box plotoutlier\n10th percentile25th percentile75th percentile50th percentile10th percentile\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nExample of Box Plots ï¬Box plots can be used to compare attributes\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization Techniques: Scatter Plotsï¬Scatter plots â€“Attributes values determine the positionâ€“Two-dimensional scatter plots most common, but can have three-dimensional scatter plotsâ€“Often additional attributes can be "}
{"text": "iques: Scatter Plotsï¬Scatter plots â€“Attributes values determine the positionâ€“Two-dimensional scatter plots most common, but can have three-dimensional scatter plotsâ€“Often additional attributes can be displayed by using the size, shape, and color of the markers that represent the objects â€“It is useful to have arrays of scatter plots can compactly summarize the relationships of several pairs of attributesu See example on the next slide\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nScatter Plot Array of Iris Attributes\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization Techniques: Contour Plotsï¬Contour plots â€“Useful when a continuous attribute is measured on a spatial gridâ€“They partition the plane into regions of similar valuesâ€“The contour lines that form the boundaries of these regions connect points with equal values â€“The most common example is contour maps of elevationâ€“Can also display temperature, rainfall, air pressure, etc.uAn example for Sea Surface Temperature (SST) is provided on the next slide\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Contour Plot Example: SST Dec, 1998\nCelsius\nÂ© Tan,Steinbach, Kumar Intr"}
{"text": "for Sea Surface Temperature (SST) is provided on the next slide\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Contour Plot Example: SST Dec, 1998\nCelsius\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization Techniques: Matrix Plotsï¬Matrix plots â€“Can plot the data matrixâ€“This can be useful when objects are sorted according to classâ€“Typically, the attributes are normalized to prevent one attribute from dominating the plot â€“Plots of similarity or distance matrices can also be useful for visualizing the relationships between objectsâ€“Examples of matrix plots are presented on the next two slides\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization of the Iris Data Matrix\nstandarddeviation\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization of the Iris Correlation Matrix\n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Visualization Techniques: Parallel Coordinatesï¬Parallel Coordinates â€“Used to plot the attribute values of high-dimensional dataâ€“Instead of using perpendicular axes, use a set of parallel axes â€“The attribute values of each object are plotted as a point on each c"}
{"text": "inates â€“Used to plot the attribute values of high-dimensional dataâ€“Instead of using perpendicular axes, use a set of parallel axes â€“The attribute values of each object are plotted as a point on each corresponding coordinate axis and the points are connected by a line â€“Thus, each object is represented as a line â€“Often, the lines representing a distinct class of objects group together, at least for some attributesâ€“Ordering of attributes is important in seeing such groupings\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Parallel Coordinates Plots for Iris Data\n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Other Visualization Techniquesï¬Star Plots â€“Similar approach to parallel coordinates, but axes radiate from a central pointâ€“The line connecting the values of an object is a polygonï¬Chernoff Facesâ€“Approach created by Herman Chernoffâ€“This approach associates each attribute with a characteristic of a faceâ€“The values of each attribute determine the appearance of the corresponding facial characteristic â€“Each object becomes a separate faceâ€“Relies on humanâ€™s ability to distinguish faces\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º St"}
{"text": "rance of the corresponding facial characteristic â€“Each object becomes a separate faceâ€“Relies on humanâ€™s ability to distinguish faces\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Star Plots for Iris Data\nSetosaVersicolourVirginica\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Chernoff Faces for Iris Data\nSetosaVersicolourVirginica\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º OLAPï¬On-Line Analytical Processing (OLAP) was proposed by E. F. Codd, the father of the relational database.ï¬Relational databases put data into tables, while OLAP uses a multidimensional array representation. â€“Such representations of data previously existed in statistics and other fieldsï¬There are a number of data analysis and data exploration operations that are easier with such a data representation. \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Creating a Multidimensional Arrayï¬Two key steps in converting tabular data into a multidimensional array.â€“First, identify which attributes are to be the dimensions and which attribute is to be the target attribute whose values appear as entries in the multidimensional array.uThe attributes u"}
{"text": "sional array.â€“First, identify which attributes are to be the dimensions and which attribute is to be the target attribute whose values appear as entries in the multidimensional array.uThe attributes used as dimensions must have discrete valuesuThe target value is typically a count or continuous value, e.g., the cost of an itemuCan have no target variable at all except the count of objects that have the same set of attribute valuesâ€“Second, find the value of each entry in the multidimensional array by summing the values (of the target attribute) or count of all objects that have the attribute values corresponding to that entry.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Example: Iris dataï¬We show how the attributes, petal length, petal width, and species type can be converted to a multidimensional arrayâ€“First, we discretized the petal width and length to have categorical values: low, medium, and highâ€“We get the following table - note the count attribute\n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nExample: Iris data (continued)ï¬Each unique tuple of petal width, petal length, and species type identifies one element of the array.ï¬This element"}
{"text": "bach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º \nExample: Iris data (continued)ï¬Each unique tuple of petal width, petal length, and species type identifies one element of the array.ï¬This element is assigned the corresponding count value. ï¬The figure illustrates the result.ï¬All non-specified tuples are 0.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º Example: Iris data (continued)ï¬Slices of the multidimensional array are shown by the following cross-tabulationsï¬What do these tables tell us?\n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º OLAP Operations: Data Cubeï¬The key operation of a OLAP is the formation of a data cubeï¬A data cube is a multidimensional representation of data, together with all possible aggregates.ï¬By all possible aggregates, we mean the aggregates that result by selecting a proper subset of the dimensions and summing over all remaining dimensions.ï¬For example, if we choose the species type dimension of the Iris data and sum over all other dimensions, the result will be a one-dimensional entry with three entries, each of which gives the number of flowers of each type. \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8"}
{"text": " over all other dimensions, the result will be a one-dimensional entry with three entries, each of which gives the number of flowers of each type. \nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º ï¬Consider a data set that records the sales of products at a number of company stores at various dates.ï¬This data can be represented as a 3 dimensional arrayï¬There are 3 two-dimensionalaggregates (3 choose 2 ),3 one-dimensional aggregates,and 1 zero-dimensional aggregate (the overall total)\nData Cube Example\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º ï¬The following figure table shows one of the two dimensional aggregates, along with two of the one-dimensional aggregates, and the overall totalData Cube Example (continued)\n\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º OLAP Operations: Slicing and Dicingï¬Slicing is selecting a group of cells from the entire multidimensional array by specifying a specific value for one or more dimensions. ï¬Dicing involves selecting a subset of cells by specifying a range of attribute values. â€“This is equivalent to defining a subarray from the complete array. ï¬In practice, both operations can also be ac"}
{"text": "ï¬Dicing involves selecting a subset of cells by specifying a range of attribute values. â€“This is equivalent to defining a subarray from the complete array. ï¬In practice, both operations can also be accompanied by aggregation over some dimensions.\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º OLAP Operations: Roll-up and Drill-downï¬Attribute values often have a hierarchical structure.â€“Each date is associated with a year, month, and week.â€“A location is associated with a continent, country, state (province, etc.), and city. â€“Products can be divided into various categories, such as clothing, electronics, and furniture.ï¬Note that these categories often nest and form a tree or latticeâ€“A year contains months which contains dayâ€“A country contains a state which contains a city\nÂ© Tan,Steinbach, Kumar Introduction to Data Mining 8/05/2005 â€¹#â€º OLAP Operations: Roll-up and Drill-downï¬This hierarchical structure gives rise to the roll-up and drill-down operations.â€“For sales data, we can aggregate (roll up) the sales across all the dates in a month. â€“Conversely, given a view of the data where the time dimension is broken into months, we could split the monthly sales totals (dri"}
{"text": "we can aggregate (roll up) the sales across all the dates in a month. â€“Conversely, given a view of the data where the time dimension is broken into months, we could split the monthly sales totals (drill down) into daily sales totals.â€“Likewise, we can drill down or roll up on the location or product ID attributes.\nTahapan Data Preparation\n\nData Preparation dalam CRISP -DM\nâ—Akronim dari: CRoss Industry \nStandard Process Data Mining\nâ—Metodologi umum untuk data \nmining, analitik, dan proyek data \nsains, berfungsi menstandarkan \nproses data mining lintas industri\nâ—Digunakan untuk semua level \ndari pemula hingga pakar \n\nTahapan Data Preparation: Versi Simple\nmodul # 8 DTS\nTahapan Data Preparation: Pemilihan, Pembersihan & \nValidasi\n1.Pilih/ Select Data\nâ€¢Pertimbangkan apemilihan data\nâ€¢Tentukan dataset yang akan \ndigunakan\nâ€¢Kumpulkan data tambahan yang \nsesuai (internal atau eksternal)\nâ€¢Pertimbangkan penggunaan \nteknik pengambilan sampel\nâ€¢Jelaskan mengapa data tertentu \ndimasukkan atau dikecualikan2.Bersihkan/ Clean Data\nâ€¢Perbaiki, hapus atau abaikan \nnoise\nâ€¢Putuskan bagaimana menangani \nnilai-nilai khusus dan maknanya\nâ€¢Tingkat agregasi, nilai yang hilang \n(missing value), dll\nâ€¢Bersihkan a"}
{"text": "likan2.Bersihkan/ Clean Data\nâ€¢Perbaiki, hapus atau abaikan \nnoise\nâ€¢Putuskan bagaimana menangani \nnilai-nilai khusus dan maknanya\nâ€¢Tingkat agregasi, nilai yang hilang \n(missing value), dll\nâ€¢Bersihkan atau manipulasi outlier\n3.Validasi Data\nâ€¢Periksa/Nilai Kualitas Data\nâ€¢Periksa/Nilai Tingkat Kecukupan \nData\n\nRincian Tahapan Data Preparation\n3.Bangun/ Construct Data\nâ€¢Atribut turunan.\nâ€¢Latar belakang pengetahuan.\nâ€¢Bagaimana atribut yang hilang dapat dibangun atau diperhitungkan\n4.Integrasi/ Integrate Data\nâ€¢Mengintegrasikan sumber dan menyimpan hasil (tabel dan catatan baru)\n5.Bentuk/ Format Data\n\nSampling Data: Pengertian Sampling\nâ—Sebelum melakukan tahapan dalam data preparation, terlebih dahulu adalah \npemilihan/penentuan objek yang dapat dilakukan dengan menggunakan \npenentuan:\nâ—‹Populasi \nâ—‹Sampel\nSampel SampelPopulasi\nTeknik Sampling\nSampling Data: Metode Sampling\nâ—Kategori Metode Sampling â—Probability Sampling:\nâ—‹Populasi diketahui\nâ—‹Randomisasi/keteracakan: Ya\nâ—‹Conclusiver \nâ—‹Hasil: Unbiased\nâ—‹Kesimpulan: Statistik\nâ—Non-Probability Samplimg\nâ—‹Populasi tidak diketahui\nâ—‹Keterbatasan penelitian \nâ—‹Randomisasi/keteracakan: Tidak\nâ—‹Exploratory\nâ—‹Hasil: Biased\nâ—‹Kesimpulan: Analitik\n\nSampling Da"}
{"text": "ased\nâ—‹Kesimpulan: Statistik\nâ—Non-Probability Samplimg\nâ—‹Populasi tidak diketahui\nâ—‹Keterbatasan penelitian \nâ—‹Randomisasi/keteracakan: Tidak\nâ—‹Exploratory\nâ—‹Hasil: Biased\nâ—‹Kesimpulan: Analitik\n\nSampling Data: Metode Sampling\n\nSampling Data: Teknik Sampling\n\nImbalance Dataset: Resampling\nâ—Ini dilakukan setelah proses pemilihan, pembersihan dan rekayasa fitur \ndilakuan atas pertanyaan:\nâ—‹Tanya: apakah kelas target data yang kita inginkan telah secara sama terdistribusi di seluruh \ndataset?\nâ—‹Jawab: Di banyak kasus tidak/belum tentu. Biasanya terjadi imbalance (ketidakseimbangan) \nantara dua kelas. Misal utk dataset tentang detekis fraud di perbankan, lelang real -time, atau \ndeteksi intrusi di network! Biasanya data dari dataset tersebut berukuran sangat kecil atau \nkurang dari 1%, namun sangat signifikan. Kebanyakan algoritma ML tidak bekerja baik utk \ndataset imbalance tsb. \n\nImbalance Dataset: Resampling\nâ—Berikut adalah bbrp cara utk mengatasi imbalance dataset:\nâ—‹Gunakan pengukuran (metrik) yang tepat, misal dengan menggunakan:\nâ– Precision/Spesikasi: berapa banyak instance yang relevan\nâ– Recall/Sensitifitas: berapa banyak instance yang dipilih\nâ– F1 score: harmonisasi mean dari precision dan"}
{"text": "yang tepat, misal dengan menggunakan:\nâ– Precision/Spesikasi: berapa banyak instance yang relevan\nâ– Recall/Sensitifitas: berapa banyak instance yang dipilih\nâ– F1 score: harmonisasi mean dari precision dan recall\nâ– MCC: koefisien korelasi antara klasifikasi biner antara observasi vs prediksi\nâ– AUC: relasi antara tingkat true -positive vs false -positive\nâ—‹Resample data training, dengan dua metode:\nâ– Undersampling: menyeimbangkan dataset dengan mereduksi ukuran kelas yang \nmelimpah. Dilakukan jika kuantitas data mencukupi\nâ– Oversampling: Kebalikan dari undersampling, dilakukan jika kuantitas data tidak \nmencukupi\nImbalance Dataset: Resampling\nâ—Teknik Resampling:\nâ—‹oversampling (SMOTE) \nâ—‹oversampling (Bootstrap)\nâ—‹undersampling (Bootstrap)\n\nSeleksi Fitur Data\nâ—Manfaat:\nâ—‹Reduksi Overfitting : semakin kecil data redundant maka keputusan berdasarkan noise \nsemakin berkurang\nâ—‹Meningkatkan Akurasi: semakin kecil data misleading maka akurasi model lebih baik\nâ—‹Reduksi Waktu Training: semakin kecil titik data (data point) maka kompleksitas algortima \nberkurang dan latih algoritma lebih cepat\nâ—Jenis: \nâ—‹Unsupervised: metode yang mengabaikan variabel target, seperti menghapus variabel yang \nberlebihan meng"}
{"text": "ta (data point) maka kompleksitas algortima \nberkurang dan latih algoritma lebih cepat\nâ—Jenis: \nâ—‹Unsupervised: metode yang mengabaikan variabel target, seperti menghapus variabel yang \nberlebihan menggunakan korelasi\nâ—‹Supervised: metode yang menggunakan variabel target , seperti menghapus variabel yang \ntidak relevan\nHands On: Seleksi Fitur\nâ—Matriks Korelasi dengan Heatmap\nâ—‹Korelasi menyatakan bagaimana \nfitur terkait satu sama lain atau \nvariabel target.\nâ—‹Korelasi bisa positif (kenaikan satu \nnilai fitur meningkatkan nilai variabel \ntarget) atau negatif (kenaikan satu \nnilai fitur menurunkan nilai variabel \ntarget)\nâ—‹Heatmap memudahkan untuk \nmengidentifikasi fitur mana yang \npaling terkait dengan variabel target, \nkami akan memplot peta panas fitur \nyang berkorelasi menggunakan \nseaborn library\n\nHands On: Seleksi Fitur\nâ—Matriks Korelasi dengan Heatmap \n(lanjutan)\nâ—‹lihat pada baris terakhir yaitu price range , \nkorelasi antara price range dengan fitur \nlain dimana ada relasi kuat dengan \nvariabel ram dan diikuti oleh var battery \npower , px height and px width .\nâ—‹sedangkan utk var clock_speed dan \nn_cores berkorelasi lemah dengan price \nrange"}
